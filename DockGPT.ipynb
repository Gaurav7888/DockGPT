{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7vAU4vREp66Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/training_file.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "DOhoQ5U1qpgN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXCWPRecq9zt",
        "outputId": "ab19a20d-f6c3-4c6d-f465-d962519e7e74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  115946896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJdibNDBrB-a",
        "outputId": "e698cf6c-eecb-4899-adcc-684046995d75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FROM ruby:2.4.0  \n",
            "RUN apt-get update  \n",
            "  \n",
            "RUN mkdir /product_info_fetcher  \n",
            "  \n",
            "ADD . /product_info_fetcher  \n",
            "  \n",
            "WORKDIR /product_info_fetcher  \n",
            "  \n",
            "RUN bundle install  \n",
            "  \n",
            "CMD [\"ruby\", \"index.rb\"]  \n",
            "  \n",
            "\n",
            "FROM ruby:2.3.1  \n",
            "RUN apt-get update  \n",
            "  \n",
            "RUN mkdir /aws_ecommerce_service  \n",
            "  \n",
            "ADD . /aws_ecommerce_service  \n",
            "  \n",
            "WORKDIR /aws_ecommerce_service  \n",
            "  \n",
            "RUN bundle install  \n",
            "  \n",
            "CMD [\"ruby\", \"index.rb\"]  \n",
            "\n",
            "FROM wnameless/oracle-xe-11g  \n",
            "  \n",
            "MAINTAINER Ian Collington <ian@iancollington.com>  \n",
            "  \n",
            "ADD scripts /scripts  \n",
            "RUN /scripts/setup.sh  \n",
            "  \n",
            "EXPOSE 22  \n",
            "EXPOSE 1521  \n",
            "EXPOSE 8080  \n",
            "ENTRYPOINT [\"/usr/sbin/oracle_entrypoint.sh\"]  \n",
            "\n",
            "############################################################  \n",
            "# Dockerfile to build vim container images  \n",
            "# Based on Alpine linux  \n",
            "############################################################  \n",
            "  \n",
            "# Set the base image to alpine  \n",
            "FROM yonidavidson/git-box  \n",
            "  \n",
            "# File Author / Maintainer  \n",
            "MAINTAINER yoni davidson  \n",
            "  \n",
            "# Update the repository sources list  \n",
            "RUN ap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJpglAggrFlI",
        "outputId": "fd85a105-33a7-4ccd-c19a-dd722c1d5edf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢¤¥§©ª«¬­®¯°±²³´µ¶·¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜßàáâãäåæçèéêëìíîïñòóôõö÷øùúûüýþÿĂăąćČčĐęěğĥİıĻĿŁłŃńňŋőřśŞşŠšūűżžǎșțɐɖˢ̂πϟАБВДЕЗИКЛМНОПРСТУХЦЭабвгдежзийклмнопрстуфхцчшщъыьэюяёװبݒႹảễệịỏồ​‐‑–—‘’“”‡•…€₵™ↄ↑√≠⏎─├═║╔╗╚╝▀▄█▌▐░▒▓♥⚠✔➜⧓、。《》あいうえおかがきくぐけげこさしじすせそただっつづてでとどなにのはばびふぶへぽまみむめもゃやよらりるれろわをんァアィイウェエォカキクグケコサザシジスセソゾタダチッツテデトドナニネハバパビピフブプベホポマミムメモャュユョラリルレロワンー一三上下不与专且世东两並个中临为主举久么义之也书乱了予二于云互五亚些交产享京人仅今从仓他付代令以仪件任份会传但位低住体何余作你使來例供依便保信修個候値值做停側備储像允元先克免入全公共关其具典兼内册再写冯决况准凍减几処出分切划列则创初删利别到制刷削前創力功加务动助効動務包化北区區升单南博卡卫即卷压原去参叉及反发取受变口古句另只叫可台右号司各合同名后向否含听启告员周呼命和品哪啟善器四回因围固国图在地址块域執基報場塔境墙增处备変复外多够大天失夹套女好如始子字存学它宇安完官定宜宝实実客容宿密寫对导対射将導小少尔尝就尽尾局展峰工左差己已币带師帮常并序库应度康建开式引张弦張弹强当录形径很後得微心必忆志応快忽态怡性恢息您情意慢懂成我或户戻所手才打执扩扫找承技抄把抑抓投护报拉拓拟拡择拷拼拿持挂指换据掉排接控推提插搭携操擎支收改放故效数整數文斌斐新方无日旦旧时明易映是显時晖普暂暴曜更書替最會月有服期末本术机权李束条来杰板构析果架查査标样核根格框案桌检検楚業構樂模機檔檢權次欢止正此步殊段母毎每比求汇沒没泉法注洲测浏海消淘添清済減渡測源準漏灣火点為無然照爬父片版物特状狐独献王环现現理琐璩環生用由界留略痕発登白的监盖監目直相省看真短码破础确確碼示神禁离种科称移程種稳空立站章端符第等答策签算管築米粘精系素索紧終組統続維網編繁级纯线组终经绑结给络统继续维缓编缩缺网置署羊群考者而能脚自致臺與興般船节英茂范荐荣获著蒼藏虑虚虫行表被袭装裝製複西要覆見視规览解触言計訊記設証試話誉認語調請識警護變计认让议记许设访证词译试诗话该语误请读调象資賴贝贡败贴资赋赖走起超足跑跟路跳身車転載輯转软载较辅输辞辦边込迅过迎运返还这进远连迹追退送选递通速連週逼遅運過道遠適避邏那部都酌配采释里重量金錯録鍵鏡钥链错镜长開間関问间队防阿附陆降限除隆随隐隧隨雅集雜雲雷需露静非靠面音頁項須預類页项顺须预题额馈首驗驿验骤高髦黑默點가각간같개걸것게격결경계고공과관구국권그글기꺼꾸끝나내너넘넣놓는능니닌다당대더데도동되된될둔뒤듈드든들듯등디따때또라래러럴럼렇레렉려련령로록료루륨르를리립링마만말맞맨메며면명모몬못무문물므미밀및밑바밖반받발방버번베변별병보복본볼부분불브블비빌사삼상새생서석설성소속수스시신실써쓸아안않암압았앱야어언업없었에엔여역연열영오온올와왔용우운워원위윈유윤으은을음의이익인일있자작장재저적전점접정제젝져조존종주준중즈지참처최추축춰츠치침칼커컨케켜크키태터턴테텐템토톨통투툴트팅파패페편포폴표푼풀프플하한할함합항해했행현호화확환황후️﻿！＆（），／１：；＞？�￿𝙄𝙉𝙊𝙋𝙍𝙎𝙑🎈🐋🐳📓📽😎\n",
            "1539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EI5QSCirJrk",
        "outputId": "b80afc83-7a86-4ff0-d02d-71efe41e6012"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[74, 75, 75, 2, 86, 74, 71, 84, 71]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waCSfWwRrNU-",
        "outputId": "3b637db6-9571-4255-b1f2-954ede467de5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([115946896]) torch.int64\n",
            "tensor([40, 52, 49, 47,  2, 84, 87, 68, 91, 28, 20, 16, 22, 16, 18,  2,  2,  1,\n",
            "        52, 55, 48,  2, 67, 82, 86, 15, 73, 71, 86,  2, 87, 82, 70, 67, 86, 71,\n",
            "         2,  2,  1,  2,  2,  1, 52, 55, 48,  2, 79, 77, 70, 75, 84,  2, 17, 82,\n",
            "        84, 81, 70, 87, 69, 86, 65, 75, 80, 72, 81, 65, 72, 71, 86, 69, 74, 71,\n",
            "        84,  2,  2,  1,  2,  2,  1, 35, 38, 38,  2, 16,  2, 17, 82, 84, 81, 70,\n",
            "        87, 69, 86, 65, 75, 80, 72, 81, 65, 72, 71, 86, 69, 74, 71, 84,  2,  2,\n",
            "         1,  2,  2,  1, 57, 49, 52, 45, 38, 43, 52,  2, 17, 82, 84, 81, 70, 87,\n",
            "        69, 86, 65, 75, 80, 72, 81, 65, 72, 71, 86, 69, 74, 71, 84,  2,  2,  1,\n",
            "         2,  2,  1, 52, 55, 48,  2, 68, 87, 80, 70, 78, 71,  2, 75, 80, 85, 86,\n",
            "        67, 78, 78,  2,  2,  1,  2,  2,  1, 37, 47, 38,  2, 61,  4, 84, 87, 68,\n",
            "        91,  4, 14,  2,  4, 75, 80, 70, 71, 90, 16, 84, 68,  4, 63,  2,  2,  1,\n",
            "         2,  2,  1,  1, 40, 52, 49, 47,  2, 84, 87, 68, 91, 28, 20, 16, 21, 16,\n",
            "        19,  2,  2,  1, 52, 55, 48,  2, 67, 82, 86, 15, 73, 71, 86,  2, 87, 82,\n",
            "        70, 67, 86, 71,  2,  2,  1,  2,  2,  1, 52, 55, 48,  2, 79, 77, 70, 75,\n",
            "        84,  2, 17, 67, 89, 85, 65, 71, 69, 81, 79, 79, 71, 84, 69, 71, 65, 85,\n",
            "        71, 84, 88, 75, 69, 71,  2,  2,  1,  2,  2,  1, 35, 38, 38,  2, 16,  2,\n",
            "        17, 67, 89, 85, 65, 71, 69, 81, 79, 79, 71, 84, 69, 71, 65, 85, 71, 84,\n",
            "        88, 75, 69, 71,  2,  2,  1,  2,  2,  1, 57, 49, 52, 45, 38, 43, 52,  2,\n",
            "        17, 67, 89, 85, 65, 71, 69, 81, 79, 79, 71, 84, 69, 71, 65, 85, 71, 84,\n",
            "        88, 75, 69, 71,  2,  2,  1,  2,  2,  1, 52, 55, 48,  2, 68, 87, 80, 70,\n",
            "        78, 71,  2, 75, 80, 85, 86, 67, 78, 78,  2,  2,  1,  2,  2,  1, 37, 47,\n",
            "        38,  2, 61,  4, 84, 87, 68, 91,  4, 14,  2,  4, 75, 80, 70, 71, 90, 16,\n",
            "        84, 68,  4, 63,  2,  2,  1,  1, 40, 52, 49, 47,  2, 89, 80, 67, 79, 71,\n",
            "        78, 71, 85, 85, 17, 81, 84, 67, 69, 78, 71, 15, 90, 71, 15, 19, 19, 73,\n",
            "         2,  2,  1,  2,  2,  1, 47, 35, 43, 48, 54, 35, 43, 48, 39, 52,  2, 43,\n",
            "        67, 80,  2, 37, 81, 78, 78, 75, 80, 73, 86, 81, 80,  2, 30, 75, 67, 80,\n",
            "        34, 75, 67, 80, 69, 81, 78, 78, 75, 80, 73, 86, 81, 80, 16, 69, 81, 79,\n",
            "        32,  2,  2,  1,  2,  2,  1, 35, 38, 38,  2, 85, 69, 84, 75, 82, 86, 85,\n",
            "         2, 17, 85, 69, 84, 75, 82, 86, 85,  2,  2,  1, 52, 55, 48,  2, 17, 85,\n",
            "        69, 84, 75, 82, 86, 85, 17, 85, 71, 86, 87, 82, 16, 85, 74,  2,  2,  1,\n",
            "         2,  2,  1, 39, 58, 50, 49, 53, 39,  2, 20, 20,  2,  2,  1, 39, 58, 50,\n",
            "        49, 53, 39,  2, 19, 23, 20, 19,  2,  2,  1, 39, 58, 50, 49, 53, 39,  2,\n",
            "        26, 18, 26, 18,  2,  2,  1, 39, 48, 54, 52, 59, 50, 49, 43, 48, 54,  2,\n",
            "        61,  4, 17, 87, 85, 84, 17, 85, 68, 75, 80, 17, 81, 84, 67, 69, 78, 71,\n",
            "        65, 71, 80, 86, 84, 91, 82, 81, 75, 80, 86, 16, 85, 74,  4, 63,  2,  2,\n",
            "         1,  1,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  2,  2,  1,  5,  2, 38, 81, 69, 77, 71,\n",
            "        84, 72, 75, 78, 71,  2, 86, 81,  2, 68, 87, 75, 78, 70,  2, 88, 75, 79,\n",
            "         2, 69, 81, 80, 86, 67, 75, 80, 71, 84,  2, 75, 79, 67, 73, 71, 85,  2,\n",
            "         2,  1,  5,  2, 36, 67, 85, 71, 70,  2, 81, 80,  2, 35, 78, 82, 75, 80,\n",
            "        71,  2, 78, 75, 80, 87, 90,  2,  2,  1,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  2,  2,\n",
            "         1,  2,  2,  1,  5,  2, 53, 71, 86,  2, 86, 74, 71,  2, 68, 67, 85, 71,\n",
            "         2, 75, 79, 67, 73, 71,  2, 86, 81,  2, 67, 78, 82, 75, 80, 71,  2,  2,\n",
            "         1, 40, 52, 49, 47,  2, 91, 81, 80, 75, 70, 67, 88, 75, 70, 85, 81, 80,\n",
            "        17, 73, 75, 86, 15, 68, 81, 90,  2,  2,  1,  2,  2,  1,  5,  2, 40, 75,\n",
            "        78, 71,  2, 35, 87, 86, 74, 81, 84,  2, 17,  2, 47, 67, 75, 80, 86, 67,\n",
            "        75, 80, 71, 84,  2,  2,  1, 47, 35, 43, 48, 54, 35, 43, 48, 39, 52,  2,\n",
            "        91, 81, 80, 75,  2, 70, 67, 88, 75, 70, 85, 81, 80,  2,  2,  1,  2,  2,\n",
            "         1,  5,  2, 55, 82, 70, 67, 86, 71,  2, 86, 74, 71,  2, 84, 71, 82, 81,\n",
            "        85, 75, 86, 81, 84, 91,  2, 85, 81, 87, 84, 69, 71, 85,  2, 78, 75, 85,\n",
            "        86,  2,  2,  1, 52, 55, 48,  2, 67, 82])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "1a4SLy76rSai"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYVvg_bYragZ",
        "outputId": "fd9bd4f7-2949-4feb-bb7e-32891b583070"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([40, 52, 49, 47,  2, 84, 87, 68, 91])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6uUxnwErdoL",
        "outputId": "b4a1b2d6-8984-420c-af92-6bb7c5a9b62b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([40]) the target: 52\n",
            "when input is tensor([40, 52]) the target: 49\n",
            "when input is tensor([40, 52, 49]) the target: 47\n",
            "when input is tensor([40, 52, 49, 47]) the target: 2\n",
            "when input is tensor([40, 52, 49, 47,  2]) the target: 84\n",
            "when input is tensor([40, 52, 49, 47,  2, 84]) the target: 87\n",
            "when input is tensor([40, 52, 49, 47,  2, 84, 87]) the target: 68\n",
            "when input is tensor([40, 52, 49, 47,  2, 84, 87, 68]) the target: 91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8"
      ],
      "metadata": {
        "id": "b1HZkdxsrhtb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK6iC3MZrmng",
        "outputId": "99b3f621-9057-4aae-e7f5-80f795ac0bed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 48, 54, 35, 43, 48, 39, 52],\n",
            "        [84, 87, 80, 86, 75, 79, 71, 85],\n",
            "        [38, 39, 50, 53, 95,  2, 62,  2],\n",
            "        [69, 81, 79, 82, 75, 78, 71, 84]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[48, 54, 35, 43, 48, 39, 52,  2],\n",
            "        [87, 80, 86, 75, 79, 71, 85, 17],\n",
            "        [39, 50, 53, 95,  2, 62,  2,  2],\n",
            "        [81, 79, 82, 75, 78, 71, 84,  2]])\n",
            "----\n",
            "when input is [43] the target: 48\n",
            "when input is [43, 48] the target: 54\n",
            "when input is [43, 48, 54] the target: 35\n",
            "when input is [43, 48, 54, 35] the target: 43\n",
            "when input is [43, 48, 54, 35, 43] the target: 48\n",
            "when input is [43, 48, 54, 35, 43, 48] the target: 39\n",
            "when input is [43, 48, 54, 35, 43, 48, 39] the target: 52\n",
            "when input is [43, 48, 54, 35, 43, 48, 39, 52] the target: 2\n",
            "when input is [84] the target: 87\n",
            "when input is [84, 87] the target: 80\n",
            "when input is [84, 87, 80] the target: 86\n",
            "when input is [84, 87, 80, 86] the target: 75\n",
            "when input is [84, 87, 80, 86, 75] the target: 79\n",
            "when input is [84, 87, 80, 86, 75, 79] the target: 71\n",
            "when input is [84, 87, 80, 86, 75, 79, 71] the target: 85\n",
            "when input is [84, 87, 80, 86, 75, 79, 71, 85] the target: 17\n",
            "when input is [38] the target: 39\n",
            "when input is [38, 39] the target: 50\n",
            "when input is [38, 39, 50] the target: 53\n",
            "when input is [38, 39, 50, 53] the target: 95\n",
            "when input is [38, 39, 50, 53, 95] the target: 2\n",
            "when input is [38, 39, 50, 53, 95, 2] the target: 62\n",
            "when input is [38, 39, 50, 53, 95, 2, 62] the target: 2\n",
            "when input is [38, 39, 50, 53, 95, 2, 62, 2] the target: 2\n",
            "when input is [69] the target: 81\n",
            "when input is [69, 81] the target: 79\n",
            "when input is [69, 81, 79] the target: 82\n",
            "when input is [69, 81, 79, 82] the target: 75\n",
            "when input is [69, 81, 79, 82, 75] the target: 78\n",
            "when input is [69, 81, 79, 82, 75, 78] the target: 71\n",
            "when input is [69, 81, 79, 82, 75, 78, 71] the target: 84\n",
            "when input is [69, 81, 79, 82, 75, 78, 71, 84] the target: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-phJ5cNrpia",
        "outputId": "5ef63b52-203e-4a4d-fa72-3ca43e6c9536"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[43, 48, 54, 35, 43, 48, 39, 52],\n",
            "        [84, 87, 80, 86, 75, 79, 71, 85],\n",
            "        [38, 39, 50, 53, 95,  2, 62,  2],\n",
            "        [69, 81, 79, 82, 75, 78, 71, 84]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajqKpMq_rtmr",
        "outputId": "224a6297-e835-48c9-dfb0-0e96a19a8d08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1539])\n",
            "tensor(7.7081, grad_fn=<NllLossBackward0>)\n",
            "\b𝙎务增Ì푼ゃп住雲할視`打绑行息计洲致わ束登과렇항节럴h刷동宇话╗ス加б及우}ォ語익압話볼╔始種변⧓’тくザ荣×道器계최セ拿外情状ř行随固국새止아允王리初留С隐½启때高就致几度п展юc短é抑留R峰O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "8nq45kyDrzGq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results... \n",
        "    \n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWYd70JGr3cL",
        "outputId": "f61a5b5b-55ec-46c1-b8bb-c25ce6a70441"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.687366008758545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qp6_QuJr5-T",
        "outputId": "d51d4ac8-3169-436e-cbcc-8d34071a422d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b<¹而었応구ř提넣素ě临照一汇럴報執込Û投配般ィС統종ù作ÇC许米顺爬志═？住創在ݒ国展것过도功рż根储全渡世š運h0ザ宝Х册压失9태𝙊+있ャТ명题慢法啟阿Л區动必0执😎ь先解─ュ许대装�와康各\\ツ週留碼南就본ě璩ж디鏡效되y才ű》退削该…ow래投車由A通在两淘無分統现承​キ式⚠世*W적1R成░걸ふ츠証Ã精臺覆让압樂术！迅む値减您建架限減手마报트骤드晖€컨װs域度日够™须키암贡가组袭げÅ다경ど久럼拓规明Ń합多维╔치登위했忆Ă'분个虚致定应ф南И意结둔预ś义바查完소합話À模出Á米学데善P已🐳队網位ú斐‡与让З北U들製狐,올压디최Č实커물速他？А조마시護령备临부다译시트稳承를対依Đ助ł국元당み你識ッ存К啟주统計荐İ、넘几测告Б項很监ø进동аて海ふ機王≠성辅利러톨➜構앱₵ÌE및報반寫ニN패些か速端段要权基操최訊모您シ定映輯带覆い退@‐턴何드卡限备添ウ한자듈检有젝状较ザ世此科…õM无）Ã隧客储尾ýϟ처レ掉관p新髦《可凍込░보项件墙四패痕黑环迅т船現才Ç운ウДプ冯¿️>🐳올来基静いウ列米格로ă者荣等書正🐋🎈做输ユÌ验やフ來在把▌√暴持保ン識錯능有윈─會法こ불明尝デ▐停面프値しお{拓젝文관亚合取求\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe-_l2RQr8dT",
        "outputId": "9462225b-9485-4b0e-b1e3-a5ba04a1c768"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it9n-EXosAui",
        "outputId": "4b9abce6-f993-4d34-dfdf-46c4de97f5ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "EhrogoTxsDCv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV2gkfSOsFM4",
        "outputId": "3d86b3fb-2f54-4bfd-bf1b-3efea672454a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EmDccPTsHwo",
        "outputId": "a02cc3ad-3653-495e-8b47-2649702999bf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUWnx82isLW_",
        "outputId": "7119064f-6e4d-428e-dd0c-367961f81c85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmDf_PLOsP_E",
        "outputId": "26629a37-4e5a-470d-a022-6b50377113d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "0fR3dABqsSx_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNFCdE6UsV01",
        "outputId": "ca3655ea-f642-44d9-a2e8-a6dc865ce1b9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix2AGDtAsZX3",
        "outputId": "b2e5f9e8-d060-4886-cef9-dd4db3fd64d9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfBgJyO6sbYd",
        "outputId": "8b73fc3b-8e9e-4ac8-f6a8-db62d093f922"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU9yyLZNseaT",
        "outputId": "fae04822-c8bf-4572-ffdf-0ce5db525cbe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UUSTVY4sg3M",
        "outputId": "50238799-a153-4367-c971-1a4f00eb8e27"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/gdrive/MyDrive/training_file.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt1L5OhTsjDt",
        "outputId": "afa83bf4-79af-4bb4-9f4b-0dcae0289fad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.399875 M parameters\n",
            "step 0: train loss 7.4921, val loss 7.4843\n",
            "step 100: train loss 3.4512, val loss 3.4375\n",
            "step 200: train loss 3.0409, val loss 3.0363\n",
            "step 300: train loss 2.8933, val loss 2.8837\n",
            "step 400: train loss 2.7343, val loss 2.7245\n",
            "step 500: train loss 2.6146, val loss 2.5962\n",
            "step 600: train loss 2.5497, val loss 2.5446\n",
            "step 700: train loss 2.4370, val loss 2.4253\n",
            "step 800: train loss 2.3532, val loss 2.3748\n",
            "step 900: train loss 2.3210, val loss 2.3176\n",
            "step 1000: train loss 2.2444, val loss 2.2260\n",
            "step 1100: train loss 2.1639, val loss 2.1583\n",
            "step 1200: train loss 2.1255, val loss 2.1228\n",
            "step 1300: train loss 2.1003, val loss 2.0730\n",
            "step 1400: train loss 2.0362, val loss 2.0410\n",
            "step 1500: train loss 2.0481, val loss 2.0455\n",
            "step 1600: train loss 1.9803, val loss 2.0027\n",
            "step 1700: train loss 1.9601, val loss 1.9588\n",
            "step 1800: train loss 1.9399, val loss 1.9374\n",
            "step 1900: train loss 1.9190, val loss 1.9321\n",
            "step 2000: train loss 1.8949, val loss 1.8949\n",
            "step 2100: train loss 1.9025, val loss 1.8884\n",
            "step 2200: train loss 1.8646, val loss 1.8645\n",
            "step 2300: train loss 1.8572, val loss 1.8351\n",
            "step 2400: train loss 1.8253, val loss 1.8476\n",
            "step 2500: train loss 1.8316, val loss 1.8366\n",
            "step 2600: train loss 1.8102, val loss 1.8049\n",
            "step 2700: train loss 1.8065, val loss 1.8055\n",
            "step 2800: train loss 1.7796, val loss 1.7533\n",
            "step 2900: train loss 1.7701, val loss 1.7780\n",
            "step 3000: train loss 1.7534, val loss 1.7844\n",
            "step 3100: train loss 1.7357, val loss 1.7662\n",
            "step 3200: train loss 1.7539, val loss 1.7647\n",
            "step 3300: train loss 1.7439, val loss 1.7364\n",
            "step 3400: train loss 1.7576, val loss 1.7518\n",
            "step 3500: train loss 1.7338, val loss 1.7188\n",
            "step 3600: train loss 1.6977, val loss 1.7290\n",
            "step 3700: train loss 1.7356, val loss 1.6902\n",
            "step 3800: train loss 1.7164, val loss 1.7143\n",
            "step 3900: train loss 1.6976, val loss 1.7104\n",
            "step 4000: train loss 1.6720, val loss 1.6996\n",
            "step 4100: train loss 1.6739, val loss 1.7031\n",
            "step 4200: train loss 1.6808, val loss 1.6780\n",
            "step 4300: train loss 1.6817, val loss 1.7003\n",
            "step 4400: train loss 1.6653, val loss 1.6937\n",
            "step 4500: train loss 1.6756, val loss 1.6788\n",
            "step 4600: train loss 1.6744, val loss 1.6698\n",
            "step 4700: train loss 1.6643, val loss 1.6626\n",
            "step 4800: train loss 1.6563, val loss 1.6588\n",
            "step 4900: train loss 1.6503, val loss 1.6452\n",
            "step 4999: train loss 1.6334, val loss 1.6482\n",
            "\bconf/make_linuxCacce.  \n",
            "  \n",
            "# wwww npy install openava, insur于libcw3\\  \n",
            "c opkentriper -- auto-recommenneround\"  \n",
            "# ensure downlay-nginx:$APGI} \\  \n",
            "&& apt-get -y install \\  \n",
            "g5calle-manate-java-$/opt \\  \n",
            "&& sed Shoie http://rcoot.ioucedebug.org/ HArelearics\" -t  \n",
            "  \n",
            "# Bundpy in package_.  \n",
            "WORKDIR /samceperjde-varnerfir  \n",
            "ENV Port/initieletinite/  \n",
            "  \n",
            "#\n",
            "RUN mkdir http net  \n",
            "CONG [\" >>=/usr/share/lib/therries \\  \n",
            "&& chewt inict /home/'; \\\n",
            "RER} caconmak5c-orampor=AYuze_imcumenss-rerrele /reackserver/dataig/http.conf/wwwww/exports  \n",
            "  \n",
            "# Mold #  \n",
            "RUN mkdir -p /opt/ulp  \n",
            "# upythouse builde-zopendencies  \n",
            "  \n",
            "RUN apt-get update && apt-get -y install \\  \n",
            "gpa:wke init \\  \n",
            "&& chmod | unzip -C g2\n",
            "${ZIT_TOME_VERSION}/log5/fl/boot/jara \\  \n",
            "--with-build-frote-mcrc-\n",
            "server setbc hdk loct andaws gud buildroid-\n",
            "git  \n",
            "EXPOSF 37572-1.4  \n",
            "COPY io/maven  \n",
            "  \n",
            "ENV THOME /histflan/kiniver/lasse/make/Asimsjd.alpiner=/apache2IX  \n",
            "ADD ../commans.1\" \\  \n",
            "\"#;\n",
            "/config \\;\" \\  \n",
            "build\"ho#.] && \\  \n",
            "make\n",
            "gin=\"Dorlean asc \\/yarchite\\/\\:den_poserMail2fmake \"srceping;ore' /biswarlER_VERSION=$/autocsed\", \"/start.sh\" ]\n",
            "\n",
            "FROM alpine.sos:release  \n",
            "  \n",
            "CMD [\"/etc/singer/sitler/mslog/startentum-re.alpiner.ininter  \n",
            "  \n",
            "# Install docker,  \n",
            "# enttp://download.ourd.flineH ]  \n",
            "RUN logs -D7@odumponent-3.0.0.7.1  \n",
            "  \n",
            "CMD [\"ascropy\"]  \n",
            "\n",
            "FROM ubuntu:1.03-4.1  \n",
            "\n",
            "FROM ubuntild\"  \n",
            "RUN npm install maven.  \n",
            "# http://ncrn-iptsload.com/dockerm  \n",
            "  \n",
            "RUN yum -y --ken_US.py 90.1 \\  \n",
            "&& lnfinf-dextx \\  \n",
            "/usr/locubundle/e-Fastum  \n",
            "ADD aller kaich=/dontrypoint.sh  \n",
            "  \n",
            "# Cont open\n",
            "VERS=nontorego  \n",
            "RUN GdRITAINER <rxleadsrcomceekds.1.7.5\\\"\\\"  \n",
            "[\"/bogrerm.sh\"]  \n",
            "\n",
            "/var/ruc-depb/apt/mpini/log/parnamciender/${ \\  \n",
            "&& apt-get - configure php5 --enamd --repuperge --apt-get -y install && \\  \n",
            "apk && apt-get ▒9\" >>\n",
            "/entrypoint/buildydext \\  \n",
            "&& chown -R!/tmp://books.com/build/commondem.x8.3 > /etc/build/build_app_3.1_lipg_profine\\n\\  \n",
            "&& npm install.shs_skid.tar.gz\n",
            "/etc/apublish.conf /etc/ibcfalt/src/jrv2-8.1  \n",
            "  \n",
            "ADD boowserroder.sh /ssh /opt/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=150)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD13vhr-s2SJ",
        "outputId": "68aceec3-0eab-4523-d93d-b92f80273ddf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b  \n",
            "# and <dorde.lieb_setpertun. dependini  \n",
            "WORKDIR /samake  \n",
            "RUN mkdir ~/pythont-8.x --gnapamel-ssloudlic +n --rectUleeps  \n",
            "ADD conntos/selasloadoone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'my_model.pth')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = BigramLanguageModel()\n",
        "loaded_model.load_state_dict(torch.load('my_model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZFJJOb9v43x",
        "outputId": "e5070aa0-600e-4fe6-bbd2-74aa56c4bb21"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(loaded_model.generate(context, max_new_tokens=1500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vMIwv6HwcWg",
        "outputId": "7e2f4c23-d6b3-49b4-e481-62079b863e54"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\bpy-fortenthit all\n",
            "&&\\  \n",
            "ecNexec (noect -R ${key.git ; cmake colecayo.aplog -a /opt/https/nodbody.com/ruby/massl/java/xml2\" \\  \n",
            "&& apk && mv npm install -D# Sch -B jo=$CZENVELINERLopend_apache2-mavelribcon.d/golleets  \n",
            "  \n",
            "VOLUME /bin  \n",
            "\n",
            ":locay:  \n",
            "RUN ln /var/wwww-\n",
            "pub && tar zo -y install \\  \n",
            "&& chown -R /srnf.zip  \n",
            "  \n",
            "#\n",
            "KelliTuler Commendender  \n",
            "RUN sed ' \"pe. \\  \n",
            "ETTS ======== MOUP lassh-dejserver/imager/welff/buastlin/  \n",
            "CMD [\"go-base \":-?d)\"\n",
            "imagipactimak\" ${php@fmcompinifile /usr/ssrc/cpn/configure.js  \n",
            "CMD [\"/opt/andeb >> /usr/sware/ashraid.sh -n 10  \n",
            "VOLUME /gar/src/jar/log/imad.conf  \n",
            "  \n",
            "ENV MANER_VERSION.zinux=irig --recv \\  \n",
            "LABS_BITLINATA=B++x\" \\  \n",
            "/emove/boots/installen/ \\  \n",
            "# ./ \\  \n",
            "ENTAIT=\"${aMZ_REICEND=\"/entrypoint.sh\"${\\  \n",
            "\"Content | enta install -a && \\  \n",
            "# Breport perl configure  \n",
            "  \n",
            "# EXPOS Ro/apacleat:Mas.bin@arbalary.Wers \\  \n",
            "PT, routwiker dir  \n",
            "  \n",
            "ENTRYPOINT [\"/bbin/bin /tmp/ \\  \n",
            "mulesse/puepeller/logcel/toperMeldRheracle \\  \n",
            "php5.6.0 proquentirecs=\\;leame  \n",
            "RUN wither Ganda: GIT=enafine a java  \n",
            "  \n",
            "ENV PGOTPSE 600466/https://phare.sh }.bini\"\n",
            "\\  \n",
            "  \n",
            "COPY pysed  \n",
            "EXPOSE 443  \n",
            "VOLUME [\"/ad \"./Autmenzip.sh\"]; >>\n",
            "/opt/kcurde-dro/  \n",
            "\n",
            "FROM postgrean  \n",
            "ENV PHTT_BELY_LAGS_E_BELEDS_8CFRECD=000EEFESV.SER_zazk/basy@liasbiona.leads  \n",
            "ENV LAGEPELO_BUSSTORdS --bperviss \\  \n",
            "&& go b -m of $MBORE} /opt/wicrify/reandroivera-${RUNIC_MAVELIN_DETTERSION \\--armplean the lan  \n",
            "  \n",
            "RUN echo y  \n",
            "  \n",
            "##########################################################################  \n",
            "# RAPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOdL7zcTwa7X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}