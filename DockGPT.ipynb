{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7vAU4vREp66Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/MyDrive/training_file.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "DOhoQ5U1qpgN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXCWPRecq9zt",
        "outputId": "ab19a20d-f6c3-4c6d-f465-d962519e7e74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  115946896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJdibNDBrB-a",
        "outputId": "e698cf6c-eecb-4899-adcc-684046995d75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FROM ruby:2.4.0  \n",
            "RUN apt-get update  \n",
            "  \n",
            "RUN mkdir /product_info_fetcher  \n",
            "  \n",
            "ADD . /product_info_fetcher  \n",
            "  \n",
            "WORKDIR /product_info_fetcher  \n",
            "  \n",
            "RUN bundle install  \n",
            "  \n",
            "CMD [\"ruby\", \"index.rb\"]  \n",
            "  \n",
            "\n",
            "FROM ruby:2.3.1  \n",
            "RUN apt-get update  \n",
            "  \n",
            "RUN mkdir /aws_ecommerce_service  \n",
            "  \n",
            "ADD . /aws_ecommerce_service  \n",
            "  \n",
            "WORKDIR /aws_ecommerce_service  \n",
            "  \n",
            "RUN bundle install  \n",
            "  \n",
            "CMD [\"ruby\", \"index.rb\"]  \n",
            "\n",
            "FROM wnameless/oracle-xe-11g  \n",
            "  \n",
            "MAINTAINER Ian Collington <ian@iancollington.com>  \n",
            "  \n",
            "ADD scripts /scripts  \n",
            "RUN /scripts/setup.sh  \n",
            "  \n",
            "EXPOSE 22  \n",
            "EXPOSE 1521  \n",
            "EXPOSE 8080  \n",
            "ENTRYPOINT [\"/usr/sbin/oracle_entrypoint.sh\"]  \n",
            "\n",
            "############################################################  \n",
            "# Dockerfile to build vim container images  \n",
            "# Based on Alpine linux  \n",
            "############################################################  \n",
            "  \n",
            "# Set the base image to alpine  \n",
            "FROM yonidavidson/git-box  \n",
            "  \n",
            "# File Author / Maintainer  \n",
            "MAINTAINER yoni davidson  \n",
            "  \n",
            "# Update the repository sources list  \n",
            "RUN ap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJpglAggrFlI",
        "outputId": "fd85a105-33a7-4ccd-c19a-dd722c1d5edf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â€ÂÂ—Â™Â¡Â¢Â¤Â¥Â§Â©ÂªÂ«Â¬Â­Â®Â¯Â°Â±Â²Â³Â´ÂµÂ¶Â·Â¹ÂºÂ»Â¼Â½Â¾Â¿Ã€ÃÃ‚ÃƒÃ„Ã…Ã†Ã‡ÃˆÃ‰ÃŠÃ‹ÃŒÃÃÃÃÃ‘Ã’Ã“Ã”Ã•Ã–Ã—Ã˜Ã™ÃšÃ›ÃœÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã±Ã²Ã³Ã´ÃµÃ¶Ã·Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ã¿Ä‚ÄƒÄ…Ä‡ÄŒÄÄÄ™Ä›ÄŸÄ¥Ä°Ä±Ä»Ä¿ÅÅ‚ÅƒÅ„ÅˆÅ‹Å‘Å™Å›ÅÅŸÅ Å¡Å«Å±Å¼Å¾ÇÈ™È›ÉÉ–Ë¢Ì‚Ï€ÏŸĞĞ‘Ğ’Ğ”Ğ•Ğ—Ğ˜ĞšĞ›ĞœĞĞĞŸĞ Ğ¡Ğ¢Ğ£Ğ¥Ğ¦Ğ­Ğ°Ğ±Ğ²Ğ³Ğ´ĞµĞ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑÑÑ‘×°Ø¨İ’á‚¹áº£á»…á»‡á»‹á»á»“â€‹â€â€‘â€“â€”â€˜â€™â€œâ€â€¡â€¢â€¦â‚¬â‚µâ„¢â†„â†‘âˆšâ‰ ââ”€â”œâ•â•‘â•”â•—â•šâ•â–€â–„â–ˆâ–Œâ–â–‘â–’â–“â™¥âš âœ”âœâ§“ã€ã€‚ã€Šã€‹ã‚ã„ã†ãˆãŠã‹ãŒãããã‘ã’ã“ã•ã—ã˜ã™ã›ããŸã ã£ã¤ã¥ã¦ã§ã¨ã©ãªã«ã®ã¯ã°ã³ãµã¶ã¸ã½ã¾ã¿ã‚€ã‚ã‚‚ã‚ƒã‚„ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“ã‚¡ã‚¢ã‚£ã‚¤ã‚¦ã‚§ã‚¨ã‚©ã‚«ã‚­ã‚¯ã‚°ã‚±ã‚³ã‚µã‚¶ã‚·ã‚¸ã‚¹ã‚»ã‚½ã‚¾ã‚¿ãƒ€ãƒãƒƒãƒ„ãƒ†ãƒ‡ãƒˆãƒ‰ãƒŠãƒ‹ãƒãƒãƒãƒ‘ãƒ“ãƒ”ãƒ•ãƒ–ãƒ—ãƒ™ãƒ›ãƒãƒãƒŸãƒ ãƒ¡ãƒ¢ãƒ£ãƒ¥ãƒ¦ãƒ§ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ¯ãƒ³ãƒ¼ä¸€ä¸‰ä¸Šä¸‹ä¸ä¸ä¸“ä¸”ä¸–ä¸œä¸¤ä¸¦ä¸ªä¸­ä¸´ä¸ºä¸»ä¸¾ä¹…ä¹ˆä¹‰ä¹‹ä¹Ÿä¹¦ä¹±äº†äºˆäºŒäºäº‘äº’äº”äºšäº›äº¤äº§äº«äº¬äººä»…ä»Šä»ä»“ä»–ä»˜ä»£ä»¤ä»¥ä»ªä»¶ä»»ä»½ä¼šä¼ ä½†ä½ä½ä½ä½“ä½•ä½™ä½œä½ ä½¿ä¾†ä¾‹ä¾›ä¾ä¾¿ä¿ä¿¡ä¿®å€‹å€™å€¤å€¼åšåœå´å‚™å‚¨åƒå…å…ƒå…ˆå…‹å…å…¥å…¨å…¬å…±å…³å…¶å…·å…¸å…¼å†…å†Œå†å†™å†¯å†³å†µå‡†å‡å‡å‡ å‡¦å‡ºåˆ†åˆ‡åˆ’åˆ—åˆ™åˆ›åˆåˆ åˆ©åˆ«åˆ°åˆ¶åˆ·å‰Šå‰å‰µåŠ›åŠŸåŠ åŠ¡åŠ¨åŠ©åŠ¹å‹•å‹™åŒ…åŒ–åŒ—åŒºå€å‡å•å—åšå¡å«å³å·å‹åŸå»å‚å‰åŠåå‘å–å—å˜å£å¤å¥å¦åªå«å¯å°å³å·å¸å„åˆåŒååå‘å¦å«å¬å¯å‘Šå‘˜å‘¨å‘¼å‘½å’Œå“å“ªå•Ÿå–„å™¨å››å›å› å›´å›ºå›½å›¾åœ¨åœ°å€å—åŸŸåŸ·åŸºå ±å ´å¡”å¢ƒå¢™å¢å¤„å¤‡å¤‰å¤å¤–å¤šå¤Ÿå¤§å¤©å¤±å¤¹å¥—å¥³å¥½å¦‚å§‹å­å­—å­˜å­¦å®ƒå®‡å®‰å®Œå®˜å®šå®œå®å®å®Ÿå®¢å®¹å®¿å¯†å¯«å¯¹å¯¼å¯¾å°„å°†å°å°å°‘å°”å°å°±å°½å°¾å±€å±•å³°å·¥å·¦å·®å·±å·²å¸å¸¦å¸«å¸®å¸¸å¹¶åºåº“åº”åº¦åº·å»ºå¼€å¼å¼•å¼ å¼¦å¼µå¼¹å¼ºå½“å½•å½¢å¾„å¾ˆå¾Œå¾—å¾®å¿ƒå¿…å¿†å¿—å¿œå¿«å¿½æ€æ€¡æ€§æ¢æ¯æ‚¨æƒ…æ„æ…¢æ‡‚æˆæˆ‘æˆ–æˆ·æˆ»æ‰€æ‰‹æ‰æ‰“æ‰§æ‰©æ‰«æ‰¾æ‰¿æŠ€æŠ„æŠŠæŠ‘æŠ“æŠ•æŠ¤æŠ¥æ‹‰æ‹“æ‹Ÿæ‹¡æ‹©æ‹·æ‹¼æ‹¿æŒæŒ‚æŒ‡æ¢æ®æ‰æ’æ¥æ§æ¨ææ’æ­æºæ“æ“æ”¯æ”¶æ”¹æ”¾æ•…æ•ˆæ•°æ•´æ•¸æ–‡æ–Œæ–æ–°æ–¹æ— æ—¥æ—¦æ—§æ—¶æ˜æ˜“æ˜ æ˜¯æ˜¾æ™‚æ™–æ™®æš‚æš´æ›œæ›´æ›¸æ›¿æœ€æœƒæœˆæœ‰æœæœŸæœ«æœ¬æœ¯æœºæƒææŸæ¡æ¥æ°æ¿æ„ææœæ¶æŸ¥æŸ»æ ‡æ ·æ ¸æ ¹æ ¼æ¡†æ¡ˆæ¡Œæ£€æ¤œæ¥šæ¥­æ§‹æ¨‚æ¨¡æ©Ÿæª”æª¢æ¬Šæ¬¡æ¬¢æ­¢æ­£æ­¤æ­¥æ®Šæ®µæ¯æ¯æ¯æ¯”æ±‚æ±‡æ²’æ²¡æ³‰æ³•æ³¨æ´²æµ‹æµæµ·æ¶ˆæ·˜æ·»æ¸…æ¸ˆæ¸›æ¸¡æ¸¬æºæº–æ¼ç£ç«ç‚¹ç‚ºç„¡ç„¶ç…§çˆ¬çˆ¶ç‰‡ç‰ˆç‰©ç‰¹çŠ¶ç‹ç‹¬çŒ®ç‹ç¯ç°ç¾ç†çç’©ç’°ç”Ÿç”¨ç”±ç•Œç•™ç•¥ç—•ç™ºç™»ç™½çš„ç›‘ç›–ç›£ç›®ç›´ç›¸çœçœ‹çœŸçŸ­ç ç ´ç¡€ç¡®ç¢ºç¢¼ç¤ºç¥ç¦ç¦»ç§ç§‘ç§°ç§»ç¨‹ç¨®ç¨³ç©ºç«‹ç«™ç« ç«¯ç¬¦ç¬¬ç­‰ç­”ç­–ç­¾ç®—ç®¡ç¯‰ç±³ç²˜ç²¾ç³»ç´ ç´¢ç´§çµ‚çµ„çµ±ç¶šç¶­ç¶²ç·¨ç¹çº§çº¯çº¿ç»„ç»ˆç»ç»‘ç»“ç»™ç»œç»Ÿç»§ç»­ç»´ç¼“ç¼–ç¼©ç¼ºç½‘ç½®ç½²ç¾Šç¾¤è€ƒè€…è€Œèƒ½è„šè‡ªè‡´è‡ºèˆ‡èˆˆèˆ¬èˆ¹èŠ‚è‹±èŒ‚èŒƒèè£è·è‘—è’¼è—è™‘è™šè™«è¡Œè¡¨è¢«è¢­è£…è£è£½è¤‡è¥¿è¦è¦†è¦‹è¦–è§„è§ˆè§£è§¦è¨€è¨ˆè¨Šè¨˜è¨­è¨¼è©¦è©±èª‰èªèªèª¿è«‹è­˜è­¦è­·è®Šè®¡è®¤è®©è®®è®°è®¸è®¾è®¿è¯è¯è¯‘è¯•è¯—è¯è¯¥è¯­è¯¯è¯·è¯»è°ƒè±¡è³‡è³´è´è´¡è´¥è´´èµ„èµ‹èµ–èµ°èµ·è¶…è¶³è·‘è·Ÿè·¯è·³èº«è»Šè»¢è¼‰è¼¯è½¬è½¯è½½è¾ƒè¾…è¾“è¾è¾¦è¾¹è¾¼è¿…è¿‡è¿è¿è¿”è¿˜è¿™è¿›è¿œè¿è¿¹è¿½é€€é€é€‰é€’é€šé€Ÿé€£é€±é€¼é…é‹éé“é é©é¿é‚é‚£éƒ¨éƒ½é…Œé…é‡‡é‡Šé‡Œé‡é‡é‡‘éŒ¯éŒ²éµé¡é’¥é“¾é”™é•œé•¿é–‹é–“é–¢é—®é—´é˜Ÿé˜²é˜¿é™„é™†é™é™é™¤éš†éšéšéš§éš¨é›…é›†é›œé›²é›·éœ€éœ²é™éé é¢éŸ³é é …é ˆé é¡é¡µé¡¹é¡ºé¡»é¢„é¢˜é¢é¦ˆé¦–é©—é©¿éªŒéª¤é«˜é«¦é»‘é»˜é»ê°€ê°ê°„ê°™ê°œê±¸ê²ƒê²Œê²©ê²°ê²½ê³„ê³ ê³µê³¼ê´€êµ¬êµ­ê¶Œê·¸ê¸€ê¸°êº¼ê¾¸ëë‚˜ë‚´ë„ˆë„˜ë„£ë†“ëŠ”ëŠ¥ë‹ˆë‹Œë‹¤ë‹¹ëŒ€ë”ë°ë„ë™ë˜ëœë ë‘”ë’¤ë“ˆë“œë“ ë“¤ë“¯ë“±ë””ë”°ë•Œë˜ë¼ë˜ëŸ¬ëŸ´ëŸ¼ë ‡ë ˆë ‰ë ¤ë ¨ë ¹ë¡œë¡ë£Œë£¨ë¥¨ë¥´ë¥¼ë¦¬ë¦½ë§ë§ˆë§Œë§ë§ë§¨ë©”ë©°ë©´ëª…ëª¨ëª¬ëª»ë¬´ë¬¸ë¬¼ë¯€ë¯¸ë°€ë°ë°‘ë°”ë°–ë°˜ë°›ë°œë°©ë²„ë²ˆë² ë³€ë³„ë³‘ë³´ë³µë³¸ë³¼ë¶€ë¶„ë¶ˆë¸Œë¸”ë¹„ë¹Œì‚¬ì‚¼ìƒìƒˆìƒì„œì„ì„¤ì„±ì†Œì†ìˆ˜ìŠ¤ì‹œì‹ ì‹¤ì¨ì“¸ì•„ì•ˆì•Šì•”ì••ì•˜ì•±ì•¼ì–´ì–¸ì—…ì—†ì—ˆì—ì—”ì—¬ì—­ì—°ì—´ì˜ì˜¤ì˜¨ì˜¬ì™€ì™”ìš©ìš°ìš´ì›Œì›ìœ„ìœˆìœ ìœ¤ìœ¼ì€ì„ìŒì˜ì´ìµì¸ì¼ìˆìì‘ì¥ì¬ì €ì ì „ì ì ‘ì •ì œì ì ¸ì¡°ì¡´ì¢…ì£¼ì¤€ì¤‘ì¦ˆì§€ì°¸ì²˜ìµœì¶”ì¶•ì¶°ì¸ ì¹˜ì¹¨ì¹¼ì»¤ì»¨ì¼€ì¼œí¬í‚¤íƒœí„°í„´í…Œí…í…œí† í†¨í†µíˆ¬íˆ´íŠ¸íŒ…íŒŒíŒ¨í˜í¸í¬í´í‘œí‘¼í’€í”„í”Œí•˜í•œí• í•¨í•©í•­í•´í–ˆí–‰í˜„í˜¸í™”í™•í™˜í™©í›„î«²ï¸ï»¿ï¼ï¼†ï¼ˆï¼‰ï¼Œï¼ï¼‘ï¼šï¼›ï¼ï¼Ÿï¿½ï¿¿ğ™„ğ™‰ğ™Šğ™‹ğ™ğ™ğ™‘ğŸˆğŸ‹ğŸ³ğŸ““ğŸ“½ğŸ˜\n",
            "1539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EI5QSCirJrk",
        "outputId": "b80afc83-7a86-4ff0-d02d-71efe41e6012"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[74, 75, 75, 2, 86, 74, 71, 84, 71]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waCSfWwRrNU-",
        "outputId": "3b637db6-9571-4255-b1f2-954ede467de5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([115946896]) torch.int64\n",
            "tensor([40, 52, 49, 47,  2, 84, 87, 68, 91, 28, 20, 16, 22, 16, 18,  2,  2,  1,\n",
            "        52, 55, 48,  2, 67, 82, 86, 15, 73, 71, 86,  2, 87, 82, 70, 67, 86, 71,\n",
            "         2,  2,  1,  2,  2,  1, 52, 55, 48,  2, 79, 77, 70, 75, 84,  2, 17, 82,\n",
            "        84, 81, 70, 87, 69, 86, 65, 75, 80, 72, 81, 65, 72, 71, 86, 69, 74, 71,\n",
            "        84,  2,  2,  1,  2,  2,  1, 35, 38, 38,  2, 16,  2, 17, 82, 84, 81, 70,\n",
            "        87, 69, 86, 65, 75, 80, 72, 81, 65, 72, 71, 86, 69, 74, 71, 84,  2,  2,\n",
            "         1,  2,  2,  1, 57, 49, 52, 45, 38, 43, 52,  2, 17, 82, 84, 81, 70, 87,\n",
            "        69, 86, 65, 75, 80, 72, 81, 65, 72, 71, 86, 69, 74, 71, 84,  2,  2,  1,\n",
            "         2,  2,  1, 52, 55, 48,  2, 68, 87, 80, 70, 78, 71,  2, 75, 80, 85, 86,\n",
            "        67, 78, 78,  2,  2,  1,  2,  2,  1, 37, 47, 38,  2, 61,  4, 84, 87, 68,\n",
            "        91,  4, 14,  2,  4, 75, 80, 70, 71, 90, 16, 84, 68,  4, 63,  2,  2,  1,\n",
            "         2,  2,  1,  1, 40, 52, 49, 47,  2, 84, 87, 68, 91, 28, 20, 16, 21, 16,\n",
            "        19,  2,  2,  1, 52, 55, 48,  2, 67, 82, 86, 15, 73, 71, 86,  2, 87, 82,\n",
            "        70, 67, 86, 71,  2,  2,  1,  2,  2,  1, 52, 55, 48,  2, 79, 77, 70, 75,\n",
            "        84,  2, 17, 67, 89, 85, 65, 71, 69, 81, 79, 79, 71, 84, 69, 71, 65, 85,\n",
            "        71, 84, 88, 75, 69, 71,  2,  2,  1,  2,  2,  1, 35, 38, 38,  2, 16,  2,\n",
            "        17, 67, 89, 85, 65, 71, 69, 81, 79, 79, 71, 84, 69, 71, 65, 85, 71, 84,\n",
            "        88, 75, 69, 71,  2,  2,  1,  2,  2,  1, 57, 49, 52, 45, 38, 43, 52,  2,\n",
            "        17, 67, 89, 85, 65, 71, 69, 81, 79, 79, 71, 84, 69, 71, 65, 85, 71, 84,\n",
            "        88, 75, 69, 71,  2,  2,  1,  2,  2,  1, 52, 55, 48,  2, 68, 87, 80, 70,\n",
            "        78, 71,  2, 75, 80, 85, 86, 67, 78, 78,  2,  2,  1,  2,  2,  1, 37, 47,\n",
            "        38,  2, 61,  4, 84, 87, 68, 91,  4, 14,  2,  4, 75, 80, 70, 71, 90, 16,\n",
            "        84, 68,  4, 63,  2,  2,  1,  1, 40, 52, 49, 47,  2, 89, 80, 67, 79, 71,\n",
            "        78, 71, 85, 85, 17, 81, 84, 67, 69, 78, 71, 15, 90, 71, 15, 19, 19, 73,\n",
            "         2,  2,  1,  2,  2,  1, 47, 35, 43, 48, 54, 35, 43, 48, 39, 52,  2, 43,\n",
            "        67, 80,  2, 37, 81, 78, 78, 75, 80, 73, 86, 81, 80,  2, 30, 75, 67, 80,\n",
            "        34, 75, 67, 80, 69, 81, 78, 78, 75, 80, 73, 86, 81, 80, 16, 69, 81, 79,\n",
            "        32,  2,  2,  1,  2,  2,  1, 35, 38, 38,  2, 85, 69, 84, 75, 82, 86, 85,\n",
            "         2, 17, 85, 69, 84, 75, 82, 86, 85,  2,  2,  1, 52, 55, 48,  2, 17, 85,\n",
            "        69, 84, 75, 82, 86, 85, 17, 85, 71, 86, 87, 82, 16, 85, 74,  2,  2,  1,\n",
            "         2,  2,  1, 39, 58, 50, 49, 53, 39,  2, 20, 20,  2,  2,  1, 39, 58, 50,\n",
            "        49, 53, 39,  2, 19, 23, 20, 19,  2,  2,  1, 39, 58, 50, 49, 53, 39,  2,\n",
            "        26, 18, 26, 18,  2,  2,  1, 39, 48, 54, 52, 59, 50, 49, 43, 48, 54,  2,\n",
            "        61,  4, 17, 87, 85, 84, 17, 85, 68, 75, 80, 17, 81, 84, 67, 69, 78, 71,\n",
            "        65, 71, 80, 86, 84, 91, 82, 81, 75, 80, 86, 16, 85, 74,  4, 63,  2,  2,\n",
            "         1,  1,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  2,  2,  1,  5,  2, 38, 81, 69, 77, 71,\n",
            "        84, 72, 75, 78, 71,  2, 86, 81,  2, 68, 87, 75, 78, 70,  2, 88, 75, 79,\n",
            "         2, 69, 81, 80, 86, 67, 75, 80, 71, 84,  2, 75, 79, 67, 73, 71, 85,  2,\n",
            "         2,  1,  5,  2, 36, 67, 85, 71, 70,  2, 81, 80,  2, 35, 78, 82, 75, 80,\n",
            "        71,  2, 78, 75, 80, 87, 90,  2,  2,  1,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  2,  2,\n",
            "         1,  2,  2,  1,  5,  2, 53, 71, 86,  2, 86, 74, 71,  2, 68, 67, 85, 71,\n",
            "         2, 75, 79, 67, 73, 71,  2, 86, 81,  2, 67, 78, 82, 75, 80, 71,  2,  2,\n",
            "         1, 40, 52, 49, 47,  2, 91, 81, 80, 75, 70, 67, 88, 75, 70, 85, 81, 80,\n",
            "        17, 73, 75, 86, 15, 68, 81, 90,  2,  2,  1,  2,  2,  1,  5,  2, 40, 75,\n",
            "        78, 71,  2, 35, 87, 86, 74, 81, 84,  2, 17,  2, 47, 67, 75, 80, 86, 67,\n",
            "        75, 80, 71, 84,  2,  2,  1, 47, 35, 43, 48, 54, 35, 43, 48, 39, 52,  2,\n",
            "        91, 81, 80, 75,  2, 70, 67, 88, 75, 70, 85, 81, 80,  2,  2,  1,  2,  2,\n",
            "         1,  5,  2, 55, 82, 70, 67, 86, 71,  2, 86, 74, 71,  2, 84, 71, 82, 81,\n",
            "        85, 75, 86, 81, 84, 91,  2, 85, 81, 87, 84, 69, 71, 85,  2, 78, 75, 85,\n",
            "        86,  2,  2,  1, 52, 55, 48,  2, 67, 82])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "1a4SLy76rSai"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYVvg_bYragZ",
        "outputId": "fd9bd4f7-2949-4feb-bb7e-32891b583070"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([40, 52, 49, 47,  2, 84, 87, 68, 91])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6uUxnwErdoL",
        "outputId": "b4a1b2d6-8984-420c-af92-6bb7c5a9b62b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([40]) the target: 52\n",
            "when input is tensor([40, 52]) the target: 49\n",
            "when input is tensor([40, 52, 49]) the target: 47\n",
            "when input is tensor([40, 52, 49, 47]) the target: 2\n",
            "when input is tensor([40, 52, 49, 47,  2]) the target: 84\n",
            "when input is tensor([40, 52, 49, 47,  2, 84]) the target: 87\n",
            "when input is tensor([40, 52, 49, 47,  2, 84, 87]) the target: 68\n",
            "when input is tensor([40, 52, 49, 47,  2, 84, 87, 68]) the target: 91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8"
      ],
      "metadata": {
        "id": "b1HZkdxsrhtb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK6iC3MZrmng",
        "outputId": "99b3f621-9057-4aae-e7f5-80f795ac0bed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 48, 54, 35, 43, 48, 39, 52],\n",
            "        [84, 87, 80, 86, 75, 79, 71, 85],\n",
            "        [38, 39, 50, 53, 95,  2, 62,  2],\n",
            "        [69, 81, 79, 82, 75, 78, 71, 84]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[48, 54, 35, 43, 48, 39, 52,  2],\n",
            "        [87, 80, 86, 75, 79, 71, 85, 17],\n",
            "        [39, 50, 53, 95,  2, 62,  2,  2],\n",
            "        [81, 79, 82, 75, 78, 71, 84,  2]])\n",
            "----\n",
            "when input is [43] the target: 48\n",
            "when input is [43, 48] the target: 54\n",
            "when input is [43, 48, 54] the target: 35\n",
            "when input is [43, 48, 54, 35] the target: 43\n",
            "when input is [43, 48, 54, 35, 43] the target: 48\n",
            "when input is [43, 48, 54, 35, 43, 48] the target: 39\n",
            "when input is [43, 48, 54, 35, 43, 48, 39] the target: 52\n",
            "when input is [43, 48, 54, 35, 43, 48, 39, 52] the target: 2\n",
            "when input is [84] the target: 87\n",
            "when input is [84, 87] the target: 80\n",
            "when input is [84, 87, 80] the target: 86\n",
            "when input is [84, 87, 80, 86] the target: 75\n",
            "when input is [84, 87, 80, 86, 75] the target: 79\n",
            "when input is [84, 87, 80, 86, 75, 79] the target: 71\n",
            "when input is [84, 87, 80, 86, 75, 79, 71] the target: 85\n",
            "when input is [84, 87, 80, 86, 75, 79, 71, 85] the target: 17\n",
            "when input is [38] the target: 39\n",
            "when input is [38, 39] the target: 50\n",
            "when input is [38, 39, 50] the target: 53\n",
            "when input is [38, 39, 50, 53] the target: 95\n",
            "when input is [38, 39, 50, 53, 95] the target: 2\n",
            "when input is [38, 39, 50, 53, 95, 2] the target: 62\n",
            "when input is [38, 39, 50, 53, 95, 2, 62] the target: 2\n",
            "when input is [38, 39, 50, 53, 95, 2, 62, 2] the target: 2\n",
            "when input is [69] the target: 81\n",
            "when input is [69, 81] the target: 79\n",
            "when input is [69, 81, 79] the target: 82\n",
            "when input is [69, 81, 79, 82] the target: 75\n",
            "when input is [69, 81, 79, 82, 75] the target: 78\n",
            "when input is [69, 81, 79, 82, 75, 78] the target: 71\n",
            "when input is [69, 81, 79, 82, 75, 78, 71] the target: 84\n",
            "when input is [69, 81, 79, 82, 75, 78, 71, 84] the target: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-phJ5cNrpia",
        "outputId": "5ef63b52-203e-4a4d-fa72-3ca43e6c9536"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[43, 48, 54, 35, 43, 48, 39, 52],\n",
            "        [84, 87, 80, 86, 75, 79, 71, 85],\n",
            "        [38, 39, 50, 53, 95,  2, 62,  2],\n",
            "        [69, 81, 79, 82, 75, 78, 71, 84]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajqKpMq_rtmr",
        "outputId": "224a6297-e835-48c9-dfb0-0e96a19a8d08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1539])\n",
            "tensor(7.7081, grad_fn=<NllLossBackward0>)\n",
            "\bğ™åŠ¡å¢ÃŒí‘¼ã‚ƒĞ¿ä½é›²í• è¦–`æ‰“ç»‘è¡Œæ¯è®¡æ´²è‡´ã‚æŸç™»ê³¼ë ‡í•­èŠ‚ëŸ´håˆ·ë™å®‡è¯â•—ã‚¹åŠ Ğ±åŠìš°}ã‚©èªìµì••è©±ë³¼â•”å§‹ç¨®ë³€â§“â€™Ñ‚ãã‚¶Â™è£Ã—é“å™¨ê³„ìµœã‚»æ‹¿å¤–æƒ…çŠ¶Å™è¡Œéšå›ºêµ­ìƒˆæ­¢ì•„å…ç‹ë¦¬åˆç•™Ğ¡éšÂ½å¯ë•Œé«˜å°±è‡´å‡ åº¦Ğ¿å±•ÑcçŸ­Ã©æŠ‘ç•™Rå³°O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "8nq45kyDrzGq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results... \n",
        "    \n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWYd70JGr3cL",
        "outputId": "f61a5b5b-55ec-46c1-b8bb-c25ce6a70441"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.687366008758545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qp6_QuJr5-T",
        "outputId": "d51d4ac8-3169-436e-cbcc-8d34071a422d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b<Â¹è€Œì—ˆå¿œêµ¬Å™æë„£ç´ Ä›ä¸´ç…§ä¸€æ±‡ëŸ´å ±åŸ·è¾¼Ã›æŠ•é…èˆ¬ã‚£Ğ¡çµ±ì¢…Ã¹ä½œÃ‡Cè®¸ç±³é¡ºçˆ¬å¿—â•ï¼Ÿä½å‰µåœ¨İ’å›½å±•ê²ƒè¿‡ë„åŠŸÑ€Å¼æ ¹å‚¨å…¨æ¸¡ä¸–Å¡é‹h0ã‚¶å®Ğ¥å†Œå‹å¤±9íƒœğ™Š+ìˆãƒ£Ğ¢ëª…é¢˜æ…¢æ³•å•Ÿé˜¿Ğ›å€åŠ¨å¿…0æ‰§ğŸ˜ÑŒå…ˆè§£â”€ãƒ¥è®¸ëŒ€è£…ï¿½ì™€åº·å„\\ãƒ„é€±ç•™ç¢¼å—å°±ë³¸Ä›ç’©Ğ¶ë””é¡æ•ˆë˜yæ‰Å±ã€‹é€€å‰Šè¯¥â€¦owë˜æŠ•è»Šç”±Aé€šåœ¨ä¸¤æ·˜ç„¡åˆ†çµ±ç°æ‰¿â€‹ã‚­å¼âš ä¸–*Wì 1Ræˆâ–‘ê±¸ãµì¸ è¨¼Ãƒç²¾è‡ºè¦†è®©ì••æ¨‚æœ¯ï¼è¿…ã‚€å€¤å‡æ‚¨å»ºæ¶é™æ¸›æ‰‹ë§ˆæŠ¥íŠ¸éª¤ë“œæ™–â‚¬ì»¨×°såŸŸåº¦æ—¥å¤Ÿâ„¢é¡»í‚¤ì•”è´¡ê°€ç»„è¢­ã’Ã…ë‹¤ê²½ã©ä¹…ëŸ¼æ‹“è§„æ˜Åƒí•©å¤šç»´â•”ì¹˜ç™»ìœ„í–ˆå¿†Ä‚'ë¶„ä¸ªè™šè‡´å®šåº”Ñ„å—Ğ˜æ„ç»“ë‘”é¢„Å›ä¹‰ë°”æŸ¥å®Œì†Œí•©è©±Ã€æ¨¡å‡ºÃç±³å­¦ë°å–„På·²ğŸ³é˜Ÿç¶²ä½Ãºæ–â€¡ä¸è®©Ğ—åŒ—Uë“¤è£½ç‹,ì˜¬å‹ë””ìµœÄŒå®ì»¤ë¬¼é€Ÿä»–ï¼ŸĞì¡°ë§ˆì‹œè­·ë ¹å¤‡ä¸´ë¶€ë‹¤è¯‘ì‹œíŠ¸ç¨³æ‰¿ë¥¼å¯¾ä¾ÄåŠ©Å‚êµ­å…ƒë‹¹ã¿ä½ è­˜ãƒƒå­˜Ğšå•Ÿì£¼ç»Ÿè¨ˆèÄ°ã€ë„˜å‡ æµ‹å‘ŠĞ‘é …å¾ˆç›‘Ã¸è¿›ë™Ğ°ã¦æµ·ãµæ©Ÿç‹â‰ ì„±è¾…åˆ©ëŸ¬í†¨âœæ§‹ì•±â‚µÃŒEë°å ±ë°˜å¯«ãƒ‹NíŒ¨äº›ã‹é€Ÿç«¯æ®µè¦æƒåŸºæ“ìµœè¨Šëª¨æ‚¨ã‚·å®šæ˜ è¼¯å¸¦è¦†ã„é€€@â€í„´ä½•ë“œå¡é™å¤‡æ·»ã‚¦í•œìë“ˆæ£€æœ‰ì çŠ¶è¾ƒã‚¶ä¸–æ­¤ç§‘â€¦ÃµMæ— ï¼‰Ãƒéš§å®¢å‚¨å°¾Ã½ÏŸì²˜ãƒ¬æ‰ê´€pæ–°é«¦ã€Šå¯å‡è¾¼â–‘ë³´é¡¹ä»¶å¢™å››íŒ¨ç—•é»‘ç¯è¿…Ñ‚èˆ¹ç¾æ‰Ã‡ìš´ã‚¦Ğ”ãƒ—å†¯Â¿ï¸>ğŸ³ì˜¬æ¥åŸºé™ã„ã‚¦åˆ—ç±³æ ¼ë¡œÄƒè€…è£ç­‰æ›¸æ­£ğŸ‹ğŸˆåšè¾“ãƒ¦ÃŒéªŒã‚„ãƒ•ä¾†åœ¨æŠŠâ–Œâˆšæš´æŒä¿ãƒ³è­˜éŒ¯ëŠ¥æœ‰ìœˆâ”€æœƒæ³•ã“ë¶ˆæ˜å°ãƒ‡â–åœé¢í”„å€¤ã—ãŠ{æ‹“ì æ–‡ê´€äºšåˆå–æ±‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe-_l2RQr8dT",
        "outputId": "9462225b-9485-4b0e-b1e3-a5ba04a1c768"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it9n-EXosAui",
        "outputId": "4b9abce6-f993-4d34-dfdf-46c4de97f5ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "EhrogoTxsDCv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV2gkfSOsFM4",
        "outputId": "3d86b3fb-2f54-4bfd-bf1b-3efea672454a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EmDccPTsHwo",
        "outputId": "a02cc3ad-3653-495e-8b47-2649702999bf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUWnx82isLW_",
        "outputId": "7119064f-6e4d-428e-dd0c-367961f81c85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmDf_PLOsP_E",
        "outputId": "26629a37-4e5a-470d-a022-6b50377113d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "0fR3dABqsSx_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNFCdE6UsV01",
        "outputId": "ca3655ea-f642-44d9-a2e8-a6dc865ce1b9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix2AGDtAsZX3",
        "outputId": "b2e5f9e8-d060-4886-cef9-dd4db3fd64d9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfBgJyO6sbYd",
        "outputId": "8b73fc3b-8e9e-4ac8-f6a8-db62d093f922"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU9yyLZNseaT",
        "outputId": "fae04822-c8bf-4572-ffdf-0ce5db525cbe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UUSTVY4sg3M",
        "outputId": "50238799-a153-4367-c971-1a4f00eb8e27"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/gdrive/MyDrive/training_file.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt1L5OhTsjDt",
        "outputId": "afa83bf4-79af-4bb4-9f4b-0dcae0289fad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.399875 M parameters\n",
            "step 0: train loss 7.4921, val loss 7.4843\n",
            "step 100: train loss 3.4512, val loss 3.4375\n",
            "step 200: train loss 3.0409, val loss 3.0363\n",
            "step 300: train loss 2.8933, val loss 2.8837\n",
            "step 400: train loss 2.7343, val loss 2.7245\n",
            "step 500: train loss 2.6146, val loss 2.5962\n",
            "step 600: train loss 2.5497, val loss 2.5446\n",
            "step 700: train loss 2.4370, val loss 2.4253\n",
            "step 800: train loss 2.3532, val loss 2.3748\n",
            "step 900: train loss 2.3210, val loss 2.3176\n",
            "step 1000: train loss 2.2444, val loss 2.2260\n",
            "step 1100: train loss 2.1639, val loss 2.1583\n",
            "step 1200: train loss 2.1255, val loss 2.1228\n",
            "step 1300: train loss 2.1003, val loss 2.0730\n",
            "step 1400: train loss 2.0362, val loss 2.0410\n",
            "step 1500: train loss 2.0481, val loss 2.0455\n",
            "step 1600: train loss 1.9803, val loss 2.0027\n",
            "step 1700: train loss 1.9601, val loss 1.9588\n",
            "step 1800: train loss 1.9399, val loss 1.9374\n",
            "step 1900: train loss 1.9190, val loss 1.9321\n",
            "step 2000: train loss 1.8949, val loss 1.8949\n",
            "step 2100: train loss 1.9025, val loss 1.8884\n",
            "step 2200: train loss 1.8646, val loss 1.8645\n",
            "step 2300: train loss 1.8572, val loss 1.8351\n",
            "step 2400: train loss 1.8253, val loss 1.8476\n",
            "step 2500: train loss 1.8316, val loss 1.8366\n",
            "step 2600: train loss 1.8102, val loss 1.8049\n",
            "step 2700: train loss 1.8065, val loss 1.8055\n",
            "step 2800: train loss 1.7796, val loss 1.7533\n",
            "step 2900: train loss 1.7701, val loss 1.7780\n",
            "step 3000: train loss 1.7534, val loss 1.7844\n",
            "step 3100: train loss 1.7357, val loss 1.7662\n",
            "step 3200: train loss 1.7539, val loss 1.7647\n",
            "step 3300: train loss 1.7439, val loss 1.7364\n",
            "step 3400: train loss 1.7576, val loss 1.7518\n",
            "step 3500: train loss 1.7338, val loss 1.7188\n",
            "step 3600: train loss 1.6977, val loss 1.7290\n",
            "step 3700: train loss 1.7356, val loss 1.6902\n",
            "step 3800: train loss 1.7164, val loss 1.7143\n",
            "step 3900: train loss 1.6976, val loss 1.7104\n",
            "step 4000: train loss 1.6720, val loss 1.6996\n",
            "step 4100: train loss 1.6739, val loss 1.7031\n",
            "step 4200: train loss 1.6808, val loss 1.6780\n",
            "step 4300: train loss 1.6817, val loss 1.7003\n",
            "step 4400: train loss 1.6653, val loss 1.6937\n",
            "step 4500: train loss 1.6756, val loss 1.6788\n",
            "step 4600: train loss 1.6744, val loss 1.6698\n",
            "step 4700: train loss 1.6643, val loss 1.6626\n",
            "step 4800: train loss 1.6563, val loss 1.6588\n",
            "step 4900: train loss 1.6503, val loss 1.6452\n",
            "step 4999: train loss 1.6334, val loss 1.6482\n",
            "\bconf/make_linuxCacce.  \n",
            "  \n",
            "# wwww npy install openava, insuräºlibcw3\\  \n",
            "c opkentriper -- auto-recommenneround\"  \n",
            "# ensure downlay-nginx:$APGI} \\  \n",
            "&& apt-get -y install \\  \n",
            "g5calle-manate-java-$/opt \\  \n",
            "&& sed Shoie http://rcoot.ioucedebug.org/ HArelearics\" -t  \n",
            "  \n",
            "# Bundpy in package_.  \n",
            "WORKDIR /samceperjde-varnerfir  \n",
            "ENV Port/initieletinite/  \n",
            "  \n",
            "#\n",
            "RUN mkdir http net  \n",
            "CONG [\" >>=/usr/share/lib/therries \\  \n",
            "&& chewt inict /home/'; \\\n",
            "RER} caconmak5c-orampor=AYuze_imcumenss-rerrele /reackserver/dataig/http.conf/wwwww/exports  \n",
            "  \n",
            "# Mold #  \n",
            "RUN mkdir -p /opt/ulp  \n",
            "# upythouse builde-zopendencies  \n",
            "  \n",
            "RUN apt-get update && apt-get -y install \\  \n",
            "gpa:wke init \\  \n",
            "&& chmod | unzip -C g2\n",
            "${ZIT_TOME_VERSION}/log5/fl/boot/jara \\  \n",
            "--with-build-frote-mcrc-\n",
            "server setbc hdk loct andaws gud buildroid-\n",
            "git  \n",
            "EXPOSF 37572-1.4  \n",
            "COPY io/maven  \n",
            "  \n",
            "ENV THOME /histflan/kiniver/lasse/make/Asimsjd.alpiner=/apache2IX  \n",
            "ADD ../commans.1\" \\  \n",
            "\"#;\n",
            "/config \\;\" \\  \n",
            "build\"ho#.] && \\  \n",
            "make\n",
            "gin=\"Dorlean asc \\/yarchite\\/\\:den_poserMail2fmake \"srceping;ore' /biswarlER_VERSION=$/autocsed\", \"/start.sh\" ]\n",
            "\n",
            "FROM alpine.sos:release  \n",
            "  \n",
            "CMD [\"/etc/singer/sitler/mslog/startentum-re.alpiner.ininter  \n",
            "  \n",
            "# Install docker,  \n",
            "# enttp://download.ourd.flineH ]  \n",
            "RUN logs -D7@odumponent-3.0.0.7.1  \n",
            "  \n",
            "CMD [\"ascropy\"]  \n",
            "\n",
            "FROM ubuntu:1.03-4.1  \n",
            "\n",
            "FROM ubuntild\"  \n",
            "RUN npm install maven.  \n",
            "# http://ncrn-iptsload.com/dockerm  \n",
            "  \n",
            "RUN yum -y --ken_US.py 90.1 \\  \n",
            "&& lnfinf-dextx \\  \n",
            "/usr/locubundle/e-Fastum  \n",
            "ADD aller kaich=/dontrypoint.sh  \n",
            "  \n",
            "# Cont open\n",
            "VERS=nontorego  \n",
            "RUN GdRITAINER <rxleadsrcomceekds.1.7.5\\\"\\\"  \n",
            "[\"/bogrerm.sh\"]  \n",
            "\n",
            "/var/ruc-depb/apt/mpini/log/parnamciender/${ \\  \n",
            "&& apt-get - configure php5 --enamd --repuperge --apt-get -y install && \\  \n",
            "apk && apt-get â–’9\" >>\n",
            "/entrypoint/buildydext \\  \n",
            "&& chown -R!/tmp://books.com/build/commondem.x8.3 > /etc/build/build_app_3.1_lipg_profine\\n\\  \n",
            "&& npm install.shs_skid.tar.gz\n",
            "/etc/apublish.conf /etc/ibcfalt/src/jrv2-8.1  \n",
            "  \n",
            "ADD boowserroder.sh /ssh /opt/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=150)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD13vhr-s2SJ",
        "outputId": "68aceec3-0eab-4523-d93d-b92f80273ddf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b  \n",
            "# and <dorde.lieb_setpertun. dependini  \n",
            "WORKDIR /samake  \n",
            "RUN mkdir ~/pythont-8.x --gnapamel-ssloudlic +n --rectUleeps  \n",
            "ADD conntos/selasloadoone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'my_model.pth')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = BigramLanguageModel()\n",
        "loaded_model.load_state_dict(torch.load('my_model.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZFJJOb9v43x",
        "outputId": "e5070aa0-600e-4fe6-bbd2-74aa56c4bb21"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(loaded_model.generate(context, max_new_tokens=1500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vMIwv6HwcWg",
        "outputId": "7e2f4c23-d6b3-49b4-e481-62079b863e54"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\bpy-fortenthit all\n",
            "&&\\  \n",
            "ecNexec (noect -R ${key.git ; cmake colecayo.aplog -a /opt/https/nodbody.com/ruby/massl/java/xml2\" \\  \n",
            "&& apk && mv npm install -D# Sch -B jo=$CZENVELINERLopend_apache2-mavelribcon.d/golleets  \n",
            "  \n",
            "VOLUME /bin  \n",
            "\n",
            ":locay:  \n",
            "RUN ln /var/wwww-\n",
            "pub && tar zo -y install \\  \n",
            "&& chown -R /srnf.zip  \n",
            "  \n",
            "#\n",
            "KelliTuler Commendender  \n",
            "RUN sed ' \"pe. \\  \n",
            "ETTS ======== MOUP lassh-dejserver/imager/welff/buastlin/  \n",
            "CMD [\"go-base \":-?d)\"\n",
            "imagipactimak\" ${php@fmcompinifile /usr/ssrc/cpn/configure.js  \n",
            "CMD [\"/opt/andeb >> /usr/sware/ashraid.sh -n 10  \n",
            "VOLUME /gar/src/jar/log/imad.conf  \n",
            "  \n",
            "ENV MANER_VERSION.zinux=irig --recv \\  \n",
            "LABS_BITLINATA=B++x\" \\  \n",
            "/emove/boots/installen/ \\  \n",
            "# ./ \\  \n",
            "ENTAIT=\"${aMZ_REICEND=\"/entrypoint.sh\"${\\  \n",
            "\"Content | enta install -a && \\  \n",
            "# Breport perl configure  \n",
            "  \n",
            "# EXPOS Ro/apacleat:Mas.bin@arbalary.Wers \\  \n",
            "PT, routwiker dir  \n",
            "  \n",
            "ENTRYPOINT [\"/bbin/bin /tmp/ \\  \n",
            "mulesse/puepeller/logcel/toperMeldRheracle \\  \n",
            "php5.6.0 proquentirecs=\\;leame  \n",
            "RUN wither Ganda: GIT=enafine a java  \n",
            "  \n",
            "ENV PGOTPSE 600466/https://phare.sh }.bini\"\n",
            "\\  \n",
            "  \n",
            "COPY pysed  \n",
            "EXPOSE 443  \n",
            "VOLUME [\"/ad \"./Autmenzip.sh\"]; >>\n",
            "/opt/kcurde-dro/  \n",
            "\n",
            "FROM postgrean  \n",
            "ENV PHTT_BELY_LAGS_E_BELEDS_8CFRECD=000EEFESV.SER_zazk/basy@liasbiona.leads  \n",
            "ENV LAGEPELO_BUSSTORdS --bperviss \\  \n",
            "&& go b -m of $MBORE} /opt/wicrify/reandroivera-${RUNIC_MAVELIN_DETTERSION \\--armplean the lan  \n",
            "  \n",
            "RUN echo y  \n",
            "  \n",
            "##########################################################################  \n",
            "# RAPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOdL7zcTwa7X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}